<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Design by Free CSS Templates
http://www.freecsstemplates.org
Released for free under a Creative Commons Attribution 2.5 License

Name       : Brown Stone  
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20100928

-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>Hao's blog - gensim</title>
<link href="/theme/css/style.css" rel="stylesheet" type="text/css" media="screen" />
<link href="/" type="application/atom+xml" rel="alternate" title="Hao's blog ATOM Feed" />
</head>
<body>
<div id="wrapper">
	<div id="page">
		<div id="page-bgtop">
			<div id="page-bgbtm">
 
        
				<div id="content">
					<div class="post">
						<h2 class="title"><a href="/gensimzuo-zhu-ti-mo-xing.html">gensim做主题模型</a></h2>
						<p class="meta"><span class="date">Le  </span><span class="posted">Par <a href="#">HAO</a></span><span>&nbsp; | Catégorie : <a href="/category/wen-ben-wa-jue.html">文本挖掘</a></span></p>
					<p class="meta">Tags : <span><a href="/tag/gensim.html">gensim</a> / </span>
<span><a href="/tag/ubuntu.html">Ubuntu</a> / </span>
<span><a href="/tag/python.html">Python</a> / </span>
</p>
						<div style="clear: both;">&nbsp;</div>
						<div class="entry">
						 <p>作为python的一个库，gensim给了文本主题模型足够的方便，像他自己的介绍一样，topic modelling for humans</p>
<p>具体的tutorial可以参看他的官方网页，当然是全英文的，http://radimrehurek.com/gensim/tutorial.html</p>
<p>由于这个链接打开速度太慢太慢，我决定写个中文总结：（文章参考了52nlp的博客，参看http://www.52nlp.cn）</p>
<p>安装就不用说了，在ubuntu环境下，sudo easy_install gensim即可</p>
<p>首先，引用gensim包，gensim包中引用corpora,models, similarities，分别做语料库建立，模型库和相似度比较库，后面可以看到例子</p>
<p>from gensim import corpora, models, similarities</p>
<p>我调用了结巴分词做中文处理，所以同样</p>
<p>import jieba</p>
<p>手工写个文本列表</p>
<p>sentences = ["我喜欢吃土豆","土豆是个百搭的东西","我不喜欢今天雾霾的北京"]</p>
<p>用结巴分词后待用，因为gensim包做主题模型，在意的是语料库，所以，中文英文，one-term，two-term都是无所谓的，如果有已经生成好的语料库，那么可以考虑直接跳到建模环节</p>
<p>官方提供的语料库范例是这样的：</p>
<p>corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],</p>
<blockquote>
<blockquote>
<blockquote>
<div class="highlight"><pre>      <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)],</span>
      <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)],</span>
      <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)],</span>
      <span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)],</span>
      <span class="p">[(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)],</span>
      <span class="p">[(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)],</span>
      <span class="p">[(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)],</span>
      <span class="p">[(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)]]</span>
</pre></div>


<p>每个中括号代表一句话，用逗号隔开，（0，1.0）代表词典中编号为0的词出现了一次，以此类推，很好理解</p>
</blockquote>
</blockquote>
</blockquote>
<p>回到过程中来，将范例的语句分词</p>
<p>words=[]
for doc in sentences:
    words.append(list(jieba.cut(doc)))
print words</p>
<p>输出：</p>
<p>[[u'\u6211', u'\u559c\u6b22', u'\u5403', u'\u571f\u8c46'], [u'\u571f\u8c46', u'\u662f', u'\u4e2a', u'\u767e', u'\u642d', u'\u7684', u'\u4e1c\u897f'], [u'\u6211', u'\u4e0d', u'\u559c\u6b22', u'\u4eca\u5929', u'\u96fe', u'\u973e', u'\u7684', u'\u5317\u4eac']]</p>
<p>此时输出的格式为unicode，不影响后期运算，因此我保留不变，如果想看分词结果可以用循环输出jieba分词结果</p>
<p>得到的分词结果构造词典</p>
<p>dic = corpora.Dictionary(words)
print dic
print dic.token2id</p>
<p>输出：</p>
<p>Dictionary(15 unique tokens)
{'\xe5\x8c\x97\xe4\xba\xac': 12, '\xe6\x90\xad': 6, '\xe7\x9a\x84': 9, '\xe5\x96\x9c\xe6\xac\xa2': 1, '\xe4\xb8\x8d': 10, '\xe4\xb8\x9c\xe8\xa5\xbf': 4, '\xe5\x9c\x9f\xe8\xb1\x86': 2, '\xe9\x9c\xbe': 14, '\xe6\x98\xaf': 7, '\xe4\xb8\xaa': 5, '\xe9\x9b\xbe': 13, '\xe7\x99\xbe': 8, '\xe4\xbb\x8a\xe5\xa4\xa9': 11, '\xe6\x88\x91': 3, '\xe5\x90\x83': 0}
可以看到各个词或词组在字典中的编号</p>
<p>为了方便看，我给了个循环输出：</p>
<p>for word,index in dic.token2id.iteritems():
    print word +" 编号为:"+ str(index)</p>
<p>输出：</p>
<p>北京 编号为:12
搭 编号为:6
的 编号为:9
喜欢 编号为:1
不 编号为:10
东西 编号为:4
土豆 编号为:2
霾 编号为:14
是 编号为:7
个 编号为:5
雾 编号为:13
百 编号为:8
今天 编号为:11
我 编号为:3
吃 编号为:0</p>
<p>为什么是乱序，我也不知道</p>
<p>词典生成好之后，就开始生成语料库了</p>
<p>corpus = [dic.doc2bow(text) for text in words]
print corpus</p>
<p>输出：</p>
<p>[[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(1, 1), (3, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]]</p>
<p>语料库的官方描述写的是向量空间模型格式的语料库，Corpus is simply an object which, when iterated over, returns its documents represented as sparse vectors.</p>
<p>官方说明给了一个tips：</p>
<p>In this example, the whole corpus is stored in memory, as a Python list. However,the corpus interface only dictates that a corpus must support iteration over its constituent documents. For very large corpora, it is advantageous to keep the corpus on disk, and access its documents sequentially, one at a time. All the operations and transformations are implemented in such a way that makes them independent of the size of the corpus, memory-wise.</p>
<p>大概意思就是说范例中的语料库是存在内存中的一个列表格式，但是接口对语料库的要求只是，支持在构建文本文件中可循环即可，因此对于很大的语料库，最好还是存在硬盘上，然后依次访问文件。这样可以不用考虑语料库的size，也避免了内存占用太多</p>
<p>此时，得到了语料库，接下来做一个TF-IDF变换</p>
<p>可以理解成 将用词频向量表示一句话 变换成为用 词的重要性向量表示一句话</p>
<p>（TF-IDF变换：评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。）</p>
<p>tfidf = models.TfidfModel(corpus)</p>
<p>vec = [(0, 1), (4, 1)]
print tfidf[vec]
corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
    print doc</p>
<p>输出：</p>
<p>[(0, 0.7071067811865475), (4, 0.7071067811865475)]
[(0, 0.8425587958192721), (1, 0.3109633824035548), (2, 0.3109633824035548), (3, 0.3109633824035548)]
[(2, 0.16073253746956623), (4, 0.4355066251613605), (5, 0.4355066251613605), (6, 0.4355066251613605), (7, 0.4355066251613605), (8, 0.4355066251613605), (9, 0.16073253746956623)]
[(1, 0.1586956620869655), (3, 0.1586956620869655), (9, 0.1586956620869655), (10, 0.42998768831312806), (11, 0.42998768831312806), (12, 0.42998768831312806), (13, 0.42998768831312806), (14, 0.42998768831312806)]</p>
<p>vec是查询文本向量，比较vec和训练中的三句话相似度</p>
<p>index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=14)
sims = index[tfidf[vec]]
print list(enumerate(sims))</p>
<p>输出：</p>
<p>[(0, 0.59577906), (1, 0.30794966), (2, 0.0)]</p>
<p>表示和第1句话相似度为59.578%，和第二句话的相似度位30.79%，第三句没有相似度，</p>
<p>我们看看vec这句话是什么：0为吃，4为东西，所以vec这句话可以是["吃东西"]或者["东西吃"]</p>
<p>而第一句话"我喜欢吃土豆","土豆是个百搭的东西"明显有相似度，而第三句话"我不喜欢今天雾霾的北京"，相似度几乎为0，至于为什么第一句比第二句更相似，就需要考虑TfIdf document representation和cosine similarity measure了</p>
<p>回到tfidf转换，接着训练LSI模型，假定三句话属于2个主题，</p>
<p>lsi = models.LsiModel(corpus_tfidf, id2word=dic, num_topics=2)
lsiout=lsi.print_topics(2)
print lsiout[0]
print lsiout[1]</p>
<p>输出：</p>
<p>0.532<em>"吃" + 0.290</em>"喜欢" + 0.290<em>"我" + 0.258</em>"土豆" + 0.253<em>"霾" + 0.253</em>"雾" + 0.253<em>"北京" + 0.253</em>"今天" + 0.253<em>"不" + 0.166</em>"东西"
0.393<em>"百" + 0.393</em>"搭" + 0.393<em>"东西" + 0.393</em>"是" + 0.393<em>"个" + -0.184</em>"霾" + -0.184<em>"雾" + -0.184</em>"北京" + -0.184<em>"今天" + -0.184</em>"不"</p>
<p>这就是基于SVD建立的两个主题模型内容</p>
<p>将文章投影到主题空间中</p>
<p>corpus_lsi = lsi[corpus_tfidf]
for doc in corpus_lsi:
    print doc</p>
<p>输出：</p>
<p>[(0, -0.70861576320682107), (1, 0.1431958007198823)]
[(0, -0.42764142348481798), (1, -0.88527674470703799)]
[(0, -0.66124862582594512), (1, 0.4190711252114323)]</p>
<p>因此第一三两句和主题一相似，第二句和主题二相似</p>
<p>同理做个LDA</p>
<p>lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=2)
ldaOut=lda.print_topics(2)
print ldaOut[0]
print ldaOut[1]
corpus_lda = lda[corpus_tfidf]
for doc in corpus_lda:
    print doc</p>
<p>得到的结果每次都变，给一次的输出：</p>
<p>0.077<em>吃 + 0.075</em>北京 + 0.075<em>雾 + 0.074</em>今天 + 0.073<em>不 + 0.072</em>霾 + 0.070<em>喜欢 + 0.068</em>我 + 0.062<em>的 + 0.061</em>土豆
0.091<em>吃 + 0.073</em>搭 + 0.073<em>土豆 + 0.073</em>个 + 0.073<em>是 + 0.072</em>百 + 0.071<em>东西 + 0.066</em>我 + 0.065<em>喜欢 + 0.059</em>霾
[(0, 0.31271095988105352), (1, 0.68728904011894654)]
[(0, 0.19957991735916861), (1, 0.80042008264083142)]
[(0, 0.80940337254233863), (1, 0.19059662745766134)]</p>
<p>第一二句和主题二相似，第三句和主题一相似</p>
<p>结论和LSI不一样，我估计这和样本数目太少，区别度不高有关，毕竟让我来区分把第一句和哪一句分在一个主题，我也不确定</p>
<p>输入一句话，查询属于LSI得到的哪个主题类型，先建立索引：</p>
<p>index = similarities.MatrixSimilarity(lsi[corpus])
query = "雾霾"
query_bow = dic.doc2bow(list(jieba.cut(query)))
print query_bow
query_lsi = lsi[query_bow]
print query_lsi</p>
<p>输出:</p>
<p>[(13, 1), (14, 1)]
[(0, 0.50670602027401368), (1, -0.3678056037187441)]</p>
<p>与第一个主题相似</p>
<p>比较和第几句话相似，用LSI得到的索引接着做，并排序输出</p>
<p>sims = index[query_lsi]
print list(enumerate(sims))
sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])
print sort_sims</p>
<p>输出：</p>
<p>[(0, 0.90161765), (1, -0.10271341), (2, 0.99058259)]
[(2, 0.99058259), (0, 0.90161765), (1, -0.10271341)]</p>
<p>可见和第二句话相似度很高，因为只有第二句话出现了雾霾两个词，可是惊讶的是和第一句话的相似度也很高，这得益于LSI模型的算法：在A和C共现，B和C共现的同时，可以找到A和B的相似度</p>
<p>在此感谢52nlp的好资源</p>	
						</div>
					</div>
					<div style="clear: both;">&nbsp;</div>
				</div>
				<div id="sidebar">
					<div id="logo">
						<h1><a href="">Hao's blog</h1>
					</div>
					<div id="menu">
						<ul>
							<li class="current_page_item"><a href="">Home</a></li>
							<li><a href="/archives.html">Archives</a></li>
						</ul>
					</div>
					<ul>
						<li>
							<h2>Catégories</h2>
							<ul>
								 <li ><a href="/category/coreseek.html">coreseek</a></li>
								 <li ><a href="/category/github.html">github</a></li>
								 <li ><a href="/category/sheng-huo.html">生活</a></li>
								 <li ><a href="/category/suan-fa.html">算法</a></li>
								 <li ><a href="/category/wen-ben-wa-jue.html">文本挖掘</a></li>
							</ul>
						</li>
						<li>
							<h2>Blogroll</h2>
							<ul>
                                                            <li><a href="http://getpelican.com/">Pelican</a></li>
                                                            <li><a href="http://python.org/">Python.org</a></li>
                                                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                                                            <li><a href="#">You can modify those links in your config file</a></li>
							</ul>
						</li>
                                                <li>
                                                        <h2>Social</h2>
                                                        <ul>
                                                            <li><a href="/" rel="alternate">Flux Atom</a></li>
                                                            <li><a href="http://blog.csdn.net/whzhcahzxh">原csdn_link</a></li>
                                                            <li><a href="http://weibo.com/itgeeks">我的微博_link</a></li>
                                                            <li><a href="http://weibo.com/itproduct">新产品平台微博_link</a></li>
                                                        </ul>
                                                </li><!-- /.social -->
						<li>
						        <h2>Tags</h2>
						        <ul>
                                                                <li><a href="/tag/coreseek.html">coreseek</a></li>
                                                                <li><a href="/tag/suan-fa.html">算法</a></li>
                                                                <li><a href="/tag/github.html">github</a></li>
                                                                <li><a href="/tag/start-2013.html">start， 2013</a></li>
                                                                <li><a href="/tag/python.html">Python</a></li>
                                                                <li><a href="/tag/mongodb.html">mongoDB</a></li>
                                                                <li><a href="/tag/tf-idf.html">TF-IDF</a></li>
                                                                <li><a href="/tag/shu-ling.html">舒凌</a></li>
                                                                <li><a href="/tag/wen-ben-wa-jue.html">文本挖掘</a></li>
                                                                <li><a href="/tag/r.html">R</a></li>
                                                                <li><a href="/tag/ubuntu.html">Ubuntu</a></li>
                                                                <li><a href="/tag/gensim.html">gensim</a></li>
						        </ul>
						</li>
						
						
					</ul>
				</div>
				<!-- end #sidebar -->
				<div style="clear: both;">&nbsp;</div>
			</div>
		</div>
	</div>
	<!-- end #page -->

<div id="footer">
	<p>Copyright (c) 2008 Sitename.com. All rights reserved. Design by <a href="http://www.freecsstemplates.org/">CSS Templates</a>.</p>
	<p>Proudly powered by <a href="http://alexis.notmyidea.org/pelican/">pelican</a>, which takes great advantages of <a href="http://python.org">python</a>.
</p>
</div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-47448544-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
<!-- end #footer -->
</body>
</html>