<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Hao's blog</title><link href="/" rel="alternate"></link><link href="/feeds/suan-fa.atom.xml" rel="self"></link><id>/</id><updated>2014-01-23T17:17:00+08:00</updated><entry><title>部分智能推荐算法总结</title><link href="/bu-fen-zhi-neng-tui-jian-suan-fa-zong-jie.html" rel="alternate"></link><updated>2014-01-23T17:17:00+08:00</updated><author><name>HAO</name></author><id>tag:,2014-01-23:bu-fen-zhi-neng-tui-jian-suan-fa-zong-jie.html</id><summary type="html">&lt;p&gt;简介
目前存在的推荐系统主要分为两种：
1.基于内容的推荐系统
方式：通过分析单个用户或资源的原始信息来进行推荐；
优点：对于稀疏性有一定的抵抗能力；
缺点：只能发现与已有兴趣相似的资源，难以挖掘新的感兴趣资源；
2.基于协同过滤的推荐系统
方式：基于历史上多个用户的访问信息对用户群体的喜好进行分析最后推荐使用者可能感兴趣的资源；
优点：能有效挖掘出新的感兴趣的资源，且无需考虑资源的表示形式；
缺点：对于稀疏性高的数据，系统性能会大大降低；&lt;/p&gt;
&lt;p&gt;图模型
大多数的推荐算法都面临数据稀疏性问题，图模型的算法能明显的改进稀疏性问题、提高推荐准确度。&lt;/p&gt;
&lt;p&gt;图模型：&lt;/p&gt;
&lt;p&gt;(图有小问题，表达意思即可)&lt;/p&gt;
&lt;p&gt;图模型的构造：&lt;/p&gt;
&lt;p&gt;用户对资源进行评分行为可以看作一种联系，表达了用户对资源的偏好。&lt;/p&gt;
&lt;p&gt;对这种联系建立模型，把用户和资源表示成图中的点，如果用户使用过某资源，则在该用户和资源之间连边，把评分作为边的权值。&lt;/p&gt;
&lt;p&gt;加入用户的背景信息：
  用户背景信息是很强的社会信息，用户的背景能够决定用户对信息资源的需求。背景信息相同的人可能对资源有相似的偏好。&lt;/p&gt;
&lt;p&gt;计算：
  使用带重启机制的随机游走算法（Random Walk with Restart)，计算一个用户到其他所有用户的相关度。&lt;/p&gt;
&lt;p&gt;RWR算法：算法从图中某个顶点出发，沿图中的边随机游走。在任意点上，算法以一定的概率随机地选择与该顶点相邻的边，沿这条边移动到下一个顶点，或以一定的概率直接回到出发点。对于一个非周期不可约的图，经过若干次随机游走过程，到达图中每一个顶点的概率值达到平稳分布，再次迭代也不改变图中的概率分布值。此时，图中每个点的概率值可以看作该顶点与出发点的联系紧密程度。&lt;/p&gt;
&lt;p&gt;推荐计算过程：&lt;/p&gt;
&lt;p&gt;1.把需要得到推荐的用户顶点作为出发顶点;&lt;/p&gt;
&lt;p&gt;2.在RWR迭代收敛之后，稳定概率值越大的顶点，与目标顶点联系越密切，越应该作为推荐项目。&lt;/p&gt;
&lt;p&gt;3.对得到的稳定概率值进行排序后，排除用户已经看过的资源，把概率值最大的前k个资源作为推荐集合。&lt;/p&gt;
&lt;p&gt;矩阵分解
矩阵分解的思路是把评分矩阵通过分解，用一个低秩的矩阵来逼近原来的评分矩阵。&lt;/p&gt;
&lt;p&gt;对原来的庞大的常常又非常稀疏的矩阵进行降维和分解，而分解后得到的矩阵都是稠密矩阵。&lt;/p&gt;
&lt;p&gt;v优点：
Ø比较容易编程实现
Ø比较低的时间和空间复杂度
Ø预测的精度比较高
Ø非常好的扩展性&lt;/p&gt;
&lt;p&gt;缺点:
Ø推荐的结果不具有很好的可解释性。
Ø付出的空间代价太大。&lt;/p&gt;
&lt;p&gt;几种矩阵分解方法：
1.PureSvd(简易)
•直接对用户评分矩阵R做SVD分解成两个矩阵，未知值填充0；
2.LatentFactor Model(学术界主流)
•实现满足一定约束条件的矩阵分解，需要构造一个优化问题；
3.NMF
•用于非负的值才有意义的情况，因为上面两个方法都有负值出现；&lt;/p&gt;
&lt;p&gt;Topic model
Topicmodel(主题模型)
其实网易的算法可以算是主题模型的一种简易展现。&lt;/p&gt;
&lt;p&gt;1.形成一个用户兴趣向量，通过记录用户的点击，来分析用户对某主题新闻的兴趣，考虑时间段流行度的加权；&lt;/p&gt;
&lt;p&gt;2.用K-means方法对目标用户推荐；
在主题模型中，主题表示一个概念、一个方面，表现为一系列相关的单词，是这些单词的条件概率。&lt;/p&gt;
&lt;p&gt;通俗来说，一个主题就好像一个“桶”，它装了若干出现概率较高的词语，这些词语和这个主题有很强的相关性。&lt;/p&gt;
&lt;p&gt;对于一段话来说，有些词语可以出自这个“桶”，有些可能来自那个“桶”，一段文本往往是若干个主题的杂合体。我们举个简单的例子，见下图：&lt;/p&gt;
&lt;p&gt;readable
normally 
主题模型(topicmodel)可以无监督地对文档和词进行分类。主题模型训练推理有两种：&lt;/p&gt;
&lt;p&gt;1.LDA&lt;/p&gt;
&lt;p&gt;2.pLSA
LDA可以将一篇文档用多个主题以概率形式组成，pLSA只有一个主题&lt;/p&gt;
&lt;p&gt;增强学习
增强学习原理：&lt;/p&gt;
&lt;p&gt;Agent通过接收每一个状态下的响应评估来做一个较合理的行为，设定一个回报函数，目标就是最大化回报的和。&lt;/p&gt;
&lt;p&gt;对于控制决策问题，有这么一种解决思路。我们设计一个回报函数（reward function），如果learning agent在决定一步后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负。
比如，四足机器人，如果他向前走了一步（接近目标），那么回报函数为正，后退为负。如果我们能够对每一步进行评价，得到相应的回报函数，那么就好办了，我们只需要找到一条回报值最大的路径（每步的回报之和最大），就认为是最佳的路径。&lt;/p&gt;
&lt;p&gt;增强学习算法用于改进主题模型：&lt;/p&gt;
&lt;p&gt;1.首先利用个性化标签数据和历史访问数据组合构建每个用户向量；
●
2.然后在学习用户向量过程中，算法基于强化学习的框架更新用户向量权值，且对较新的用户评价数据给予更高的权重，从而有效反映了用户的最新兴趣;
●
3.最后根据学习到的用户兴趣向量，结合协同过滤的思想对用户进行推荐。&lt;/p&gt;
&lt;p&gt;决策树
决策树构建比较简单：&lt;/p&gt;
&lt;p&gt;1.将所有的用户评分数据映射到&lt;like,dislike, unknown&gt;三维空间上；
●
2.从根结点开始，选择最佳的分割项，自顶向下建树；
●
3.终止条件，同时考虑以下三种：
设定树的最大深度；&lt;/p&gt;
&lt;p&gt;设定最佳分割项的误差阈值；&lt;/p&gt;
&lt;p&gt;设定当前节点的最少评分数量；&lt;/p&gt;
&lt;p&gt;集成学习&lt;/p&gt;
&lt;p&gt;集成学习算法(EnsembleLearning):&lt;/p&gt;
&lt;p&gt;将一系列学习器进行学习，并使用某种规则把各个学习结果进行整合从而获得比单个学习器更好的学习效果的一种机器学习方法。&lt;/p&gt;
&lt;p&gt;算法：
1.Boosting
2.Bagging&lt;/p&gt;
&lt;p&gt;集成学习&lt;/p&gt;
&lt;p&gt;集成学习算法应用于决策树                    随机森林&lt;/p&gt;
&lt;p&gt;随机森林算法：&lt;/p&gt;
&lt;p&gt;1.可放回抽样；
2.分类时选一小部分特征；
3.不剪枝情况下生成tree；
4.生成一片森林，分类时用每一棵树的结果vote&lt;/p&gt;
&lt;p&gt;Bagging&lt;/p&gt;
&lt;p&gt;Bagging和随机森林的思路相似，随机有放回的采样数据，建立多个训练器，最终预测函数对分类问题采用投票的方式得到结果。&lt;/p&gt;
&lt;p&gt;Boosting&lt;/p&gt;
&lt;p&gt;Boosting的思想是考虑了权重的：&lt;/p&gt;
&lt;p&gt;1.初始化时对每一个训练例赋相等的权重；
2.然后用该学算法对训练集训练多轮，每次训练后，对训练失败的训练例赋以较大的权重；
3.得到一个预测函数序列, 预测效果好的预测函数权重较大，反之较小；
4.最终的预测函数对分类问题采用有权重的投票。&lt;/p&gt;
&lt;p&gt;基于Boosting的个性化推荐算法:&lt;/p&gt;
&lt;p&gt;传统的协同过滤算法中，定位相似人群，利用KNN的方法，将评分乘以人群相似度加权值，取平均。&lt;/p&gt;
&lt;p&gt;定位相似人群是最关键的步骤，其中有传统方法和基于局部结构两种方法；&lt;/p&gt;
&lt;p&gt;传统相似度算法有：
1.余弦相似度；
●
2.正规化余弦相似度；(公式如下)
●
3.Pearson相关性；&lt;/p&gt;
&lt;p&gt;I表示商品集合&lt;/p&gt;
&lt;p&gt;r表示评分矩阵&lt;/p&gt;
&lt;p&gt;基于局部结构的相似度计算有：&lt;/p&gt;
&lt;p&gt;1.公共邻居:考虑两个用户之间共同打分的个数；
●
2.Salton指标；
●
3.Jaccard指标；
●
4.Sϕrensen算法；
●
I表示商品集合&lt;/p&gt;
&lt;p&gt;r表示评分矩阵&lt;/p&gt;
&lt;p&gt;上述众多的相似度测量方法可以产生众多的弱分类器，利用Boosting的思想来产生最优组合；&lt;/p&gt;</summary><category term="算法"></category></entry><entry><title>机器学习笔记</title><link href="/ji-qi-xue-xi-bi-ji.html" rel="alternate"></link><updated>2014-01-23T17:15:00+08:00</updated><author><name>HAO</name></author><id>tag:,2014-01-23:ji-qi-xue-xi-bi-ji.html</id><summary type="html">&lt;p&gt;机器学习分作监督学习和非监督学习：其中，&lt;/p&gt;
&lt;p&gt;监督学习是有预测结果的，最常见的算法是分类算法&lt;/p&gt;
&lt;p&gt;非监督学习是无预测结果的，最常见的算法是聚类算法&lt;/p&gt;
&lt;p&gt;梯度下降算法是用于得到极值点的方法，在线性回归中，梯度下降算法可以应用于回归过程，得到最小二乘法的结果。&lt;/p&gt;
&lt;p&gt;线性回归算法试图用线性方程来回归数据集，回归时，一个或多个outlier的存在会把回归方程畸形化，所以在做分类时，用线性回归不是一个好主意，而做logistic回归的时候，可以将输出从连续值转换为0或者1的分类值，适用于分类算法。logistic回归的时候，ML算法同样会运用梯度上升的算法得到极大值点时候的theta值，从而确定分类关系。&lt;/p&gt;
&lt;p&gt;牛顿梯度算法是梯度下降算法的优化，可以快速算法过程，在数据维度为vector的时候，每一次逼近都需要计算一次hessian矩阵的逆（二次求导矩阵），因此当维度过大的时候计算量会很大。&lt;/p&gt;
&lt;p&gt;线性回归容易欠拟合，如果当选择维度过多的时候又容易出现过拟合的情况。过拟合一般是分类算法最大的问题，因此，有很多解决算法出现，比如对于决策树算法，RF（随机森林）是一个避免过拟合的优化。&lt;/p&gt;
&lt;p&gt;discriminative model（判别式模型）包括logistic回归，SVM等，是将数据集分类的模型，而另一类generative model（产生式模型）的思想是，先分别取各类数据集，建立模型来总结各类数据，然后当新的数据出现的时候，分析他更类似于哪种模型来做区分，常见的基本都属于这类，包括高斯模型（建立多维高斯模型来描述各类模型），决策树模型，主题模型，高斯模型等等。&lt;/p&gt;
&lt;p&gt;混合高斯模型可以推出logistic回归的sigmoid分布，将高斯的概率值设为0~1，Pr(y=1|x)的值在高斯x轴上呈现sigmoid函数形状（s型）。&lt;/p&gt;
&lt;p&gt;产生式模型需要对于数据有清晰的认识，对于高斯分布，如果建立了泊松分布或者是伯努利分布，那么在模型的判别上会造成准确性的丧失，而判别模型则不需要考虑数据模型的分布。&lt;/p&gt;
&lt;p&gt;判别模型需要大量的数据支撑从而得到较为正确的分类器，而产生式模型没有这个限制。&lt;/p&gt;
&lt;p&gt;对于垃圾邮件甄别算法，离散型词袋可以用朴素贝叶斯算法进行分类，而对于因为分类从未出现的词导致概率为0/0，可以使用laplace平滑算法（即给所有的计数都加1）的方式来解决。&lt;/p&gt;
&lt;p&gt;甄别方法有两种：&lt;/p&gt;
&lt;p&gt;1.一个词是否出现为值，0和1&lt;/p&gt;
&lt;p&gt;2.记录每个词出现的概率，用多项分布的模型，将所有词的概率相乘(较优于第一种)&lt;/p&gt;
&lt;p&gt;logistic回归的核H()为e^(theta*T)，所以指数型的核都会得到线性的分类结果，而高斯分布也是指数型的核，为了得到非指数型的核，可以考虑用神经网络的方法，即用多个核形成hidden layer，然后再用一个输出核将hidden layer的输出作为输入，得到最终输出。&lt;/p&gt;
&lt;p&gt;神经网络算法不如SVM常用，而SVM也是现在最常用的算法，神经网络算法的最大问题是取得最优值的难度太大，如果是Logistic回归算法，用梯度下降算法或者ML算法可以得到一个全局的最优解，对于有隐藏层的神经网络来说，会有多个最优解的结果。&lt;/p&gt;
&lt;p&gt;神经网络得到最优解的算法是反向传播（BP）算法&lt;/p&gt;
&lt;p&gt;SVM的核是将数据集投射到高维空间中，然后SVM算法在线性可分的假设下将数据分类，当数据不是线性可分的时候，有软分类算法（L1型SVM算法--加入补偿系数；L2型SVM算法--SMO等）。&lt;/p&gt;
&lt;p&gt;用于SMO取得最佳值的算法是坐标上升算法，即，在若干未知系数求最大值的时候，改变某一个系数，固定其他系数不变，每次迭代取得最佳值。&lt;/p&gt;
&lt;p&gt;坐标上升算法相比于牛顿算法等需要迭代更多次数，但是每次迭代的计算量有可能很cheap，对于SMO算法的最优值取得有很好的效果。&lt;/p&gt;
&lt;p&gt;识别ocr或者图像的时候，比如邮编，传统的算法中神经网络算法的性能是最好的，但是SVM算法在使用多项式核和高斯核的时候，可以得到一个不逊于神经网络算法的结果，其中，SVM算法允许任意打乱pixel的顺序，不考虑几个pixel之间的相对位置关系，很有意义。&lt;/p&gt;
&lt;p&gt;识别蛋白质族的时候，也优选SVM算法，将四个字母得到4^26次方个组合形式，只统计各种氨基酸出现的次数，由于组合形式太多，考虑采用内积的方式简化（？）&lt;/p&gt;</summary><category term="算法"></category></entry></feed>